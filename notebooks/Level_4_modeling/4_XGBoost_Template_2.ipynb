{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataset\n",
    "dataset = pd.read_csv(\"bank-full.csv\", sep = \";\")\n",
    "dataset.dtypes\n",
    "\n",
    "#isolate the x and y variables\n",
    "y = dataset.iloc[:, -1].values\n",
    "X = dataset._get_numeric_data()\n",
    "    \n",
    "#split dataset into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size =0.2,\n",
    "                                                    random_state=1502)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform y factor variables\n",
    "y_train = np.where(y_train == \"yes\", 1, 0)\n",
    "y_test = np.where(y_test == \"yes\", 1, 0)\n",
    "np.mean(y_train)\n",
    "np.mean(y_test)\n",
    "\n",
    "#create xgboost matrices\n",
    "Train = xgb.DMatrix(X_train, label = y_train)\n",
    "Test = xgb.DMatrix(X_test, label = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the parameters\n",
    "parameters1 = {'learning_rate': 0.3,\n",
    "               'max_depth': 2,\n",
    "               'colsample_bytree': 1,\n",
    "               'subsample': 1,\n",
    "               'min_child_weight': 1,\n",
    "               'gamma': 0, \n",
    "               'random_state': 1502,\n",
    "               'eval_metric': \"auc\",\n",
    "               'objective': \"binary:logistic\"}\n",
    "\n",
    "#run XGBoost\n",
    "model1 = xgb.train(params = parameters1,\n",
    "                   dtrain = Train,\n",
    "                   num_boost_round = 200,\n",
    "                   evals = [(Test, \"Yes\")],\n",
    "                   verbose_eval = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRedictions\n",
    "predictions1 = model1.predict(Test)\n",
    "predictions1 = np.where(predictions1 > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion MAtrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "confusion_matrix1 = confusion_matrix(y_test, predictions1)\n",
    "print(confusion_matrix1)\n",
    "report1 = classification_report(y_test, predictions1)\n",
    "print(report1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "\n",
    "#isolate the categorical variables\n",
    "dataset_categorical = dataset.select_dtypes(exclude = \"number\")\n",
    "\n",
    "#tranform categorical variables into dummy variables\n",
    "dataset_categorical = pd.get_dummies(data = dataset_categorical,\n",
    "                                     drop_first = True)\n",
    "\n",
    "#joining numerical and categorical datasets\n",
    "final_dataset = pd.concat([X, dataset_categorical], axis = 1)\n",
    "\n",
    "#getting names of columns\n",
    "feature_columns = list(final_dataset.columns.values)\n",
    "feature_columns = feature_columns[:-1]\n",
    "\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolate the x and y variables part 2\n",
    "y = final_dataset.iloc[:, -1].values\n",
    "X = final_dataset.iloc[:, :-1].values\n",
    "    \n",
    "#split dataset into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size =0.2,\n",
    "                                                    random_state=1502)\n",
    "\n",
    "#create xgboost matrices part 2\n",
    "Train = xgb.DMatrix(X_train, label = y_train, feature_names = feature_columns)\n",
    "Test = xgb.DMatrix(X_test, label = y_test, feature_names = feature_columns)\n",
    "\n",
    "#set the parameters part 2\n",
    "parameters2 = {'learning_rate': 0.3,\n",
    "               'max_depth': 2,\n",
    "               'colsample_bytree': 1,\n",
    "               'subsample': 1,\n",
    "               'min_child_weight': 1,\n",
    "               'gamma': 0, \n",
    "               'random_state': 1502,\n",
    "               'eval_metric': \"auc\",\n",
    "               'objective': \"binary:logistic\"}\n",
    "\n",
    "#run XGBoost\n",
    "model2 = xgb.train(params = parameters2,\n",
    "                   dtrain = Train,\n",
    "                   num_boost_round = 200,\n",
    "                   evals = [(Test, \"Yes\")],\n",
    "                   verbose_eval = 50)\n",
    "\n",
    "#Predictions part 2\n",
    "predictions2 = model2.predict(Test)\n",
    "predictions2 = np.where(predictions2 > 0.5, 1, 0)\n",
    "\n",
    "#Confusion MAtrix\n",
    "confusion_matrix2 = confusion_matrix(y_test, predictions2)\n",
    "print(confusion_matrix2)\n",
    "report2 = classification_report(y_test, predictions2)\n",
    "print(report2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking how many cores we have\n",
    "import multiprocessing\n",
    "multiprocessing.cpu_count()\n",
    "\n",
    "#setting the cross validation parameters\n",
    "from sklearn.model_selection import KFold\n",
    "tune_control = KFold(n_splits = 5,\n",
    "                     shuffle = True,\n",
    "                     random_state = 1502).split(X = X_train,\n",
    "                                                y = y_train)\n",
    "                                                \n",
    "#set parameter tuning\n",
    "#set the parameters part 2\n",
    "tune_grid = {'learning_rate': [0.05, 0.3],\n",
    "               'max_depth': range(2, 9, 2),\n",
    "               'colsample_bytree': [0.5, 1],\n",
    "               'subsample': [1],\n",
    "               'min_child_weight': [1],\n",
    "               'gamma': [0], \n",
    "               'random_state': [1502],\n",
    "               'n_estimators': range(200, 2000, 200),\n",
    "               'booster': [\"gbtree\"]}                                                \n",
    "\n",
    "#State that we are doing a classification problem\n",
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier(objective = \"binary:logistic\")\n",
    "\n",
    "#Cross Validation Assembly\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = tune_grid,\n",
    "                            scoring = \"roc_auc\",\n",
    "                            n_jobs = 6,\n",
    "                            cv = tune_control,\n",
    "                            verbose = 5)\n",
    "\n",
    "#Setting evaluation parameters\n",
    "evaluation_parameters = {\"early_stopping_rounds\": 100,\n",
    "                         \"eval_metric\": \"auc\",\n",
    "                         \"eval_set\": [(X_test, y_test)]}\n",
    "\n",
    "#Hyperparameter tuning and cross validation\n",
    "tune_model = grid_search.fit(X = X_train,\n",
    "                             y = y_train,\n",
    "                             **evaluation_parameters)\n",
    "grid_search.best_params_, grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "\n",
    "#setting the cross validation parameters\n",
    "from sklearn.model_selection import KFold\n",
    "tune_control = KFold(n_splits = 5,\n",
    "                     shuffle = True,\n",
    "                     random_state = 1502).split(X = X_train,\n",
    "                                                y = y_train)\n",
    "                                                \n",
    "#set parameter tuning part 2\n",
    "tune_grid2 = {'learning_rate': [0.05],\n",
    "               'max_depth': [6],\n",
    "               'colsample_bytree': [0.5],\n",
    "               'subsample': [0.9, 1],\n",
    "               'min_child_weight': range(1,5,1),\n",
    "               'gamma': [0, 0.1], \n",
    "               'random_state': [1502],\n",
    "               'n_estimators': range(200, 2000, 200),\n",
    "               'booster': [\"gbtree\"]}                                                \n",
    "\n",
    "#Cross Validation Assembly\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search2 = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = tune_grid2,\n",
    "                            scoring = \"roc_auc\",\n",
    "                            n_jobs = 6,\n",
    "                            cv = tune_control,\n",
    "                            verbose = 5)\n",
    "\n",
    "#Hyperparameter tuning and cross validation\n",
    "tune_model2 = grid_search2.fit(X = X_train,\n",
    "                             y = y_train,\n",
    "                             **evaluation_parameters)\n",
    "grid_search2.best_params_, grid_search2.best_score_\n",
    "\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the parameters part 3\n",
    "parameters3 = {'learning_rate': 0.05,\n",
    "               'max_depth': 6,\n",
    "               'colsample_bytree': 0.5,\n",
    "               'subsample': 1,\n",
    "               'min_child_weight': 2,\n",
    "               'gamma': 0, \n",
    "               'random_state': 1502,\n",
    "               'eval_metric': \"auc\",\n",
    "               'objective': \"binary:logistic\"}\n",
    "\n",
    "#run XGBoost part 3\n",
    "model3 = xgb.train(params = parameters3,\n",
    "                   dtrain = Train,\n",
    "                   num_boost_round = 800,\n",
    "                   evals = [(Test, \"Yes\")],\n",
    "                   verbose_eval = 50)\n",
    "\n",
    "#Predictions part 3\n",
    "predictions3 = model3.predict(Test)\n",
    "predictions3 = np.where(predictions3 > 0.05, 1, 0)\n",
    "\n",
    "#Confusion Matrix\n",
    "confusion_matrix3 = confusion_matrix(y_test, predictions3)\n",
    "print(confusion_matrix3)\n",
    "report3 = classification_report(y_test, predictions3)\n",
    "print(report3)\n",
    "\n",
    "Predictions  = 9041\n",
    "Predictions_no = 7651 + 599\n",
    "predictions_yes= 502 + 291\n",
    "\n",
    "#plot importances\n",
    "xgb.plot_importance(model3, max_num_features = 10)\n",
    "\n",
    "#Preparing SHAP\n",
    "#pip install shap\n",
    "#conda install -c conda-forge shap\n",
    "import shap\n",
    "explainer = shap.TreeExplainer(model3)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "#shap plotting\n",
    "shap.summary_plot(shap_values,\n",
    "                  X_test,\n",
    "                  feature_names = feature_columns,\n",
    "                  max_display = 10)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
